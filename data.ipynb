{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pickle\n",
    "dataset=open('split_dataset','rb')\n",
    "dataset=pickle.load(dataset)\n",
    "testset=open('split_testset','rb')\n",
    "testset=pickle.load(testset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def label_get(dataset):\n",
    "    labelset=[]\n",
    "    for index,row in dataset.iterrows():\n",
    "        labelset.append(dataset.loc[index,'label'])\n",
    "    return labelset\n",
    "labelset=label_get(dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def pd_li_transform(dataset):\n",
    "    dataset_list=[]\n",
    "    for index,row in dataset.iterrows():\n",
    "        str2=\" \"\n",
    "        if dataset.loc[index,'comment_all']is None:\n",
    "            dataset_list.append(dataset.loc[index,'content'])\n",
    "        else:\n",
    "            dataset_list.append(dataset.loc[index,'content']+dataset.loc[index,'comment_all'])\n",
    "    return dataset_list\n",
    "testset_list=pd_li_transform(testset)\n",
    "dataset_list=pd_li_transform(dataset)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "def list_deal(dataset,labelset):\n",
    "    labelset2=[]\n",
    "    dataset2=[]\n",
    "    for i in range(len(dataset)):\n",
    "        if (dataset[i]!=[]):\n",
    "            dataset2.append(dataset[i])\n",
    "            labelset2.append(labelset[i])\n",
    "    return dataset2,labelset2\n",
    "dataset_list,labelset=list_deal(dataset_list,labelset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def li_str_transform(dataset_list):\n",
    "    dataset2=[]\n",
    "    for seq in dataset_list:\n",
    "        str1=' '.join(seq)\n",
    "        dataset2.append(str1)\n",
    "    return dataset2\n",
    "dataset_str=li_str_transform(dataset_list)\n",
    "testset_str=li_str_transform(testset_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "data_train[3835]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "+-----------+--------+\n",
       "| raw_chars | target |\n",
       "+-----------+--------+\n",
       "| []        | -1     |\n",
       "+-----------+--------+"
      ]
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "source": [
    "from fastNLP import DataSet\n",
    "data_train=DataSet({'raw_chars':dataset_list,'target':labelset})\n",
    "data_validation=DataSet({'raw_chars':testset_list})\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "source": [
    "for i in range(0,len(dataset_list)):\n",
    "    if len(dataset_list[i])<3:print(dataset_list[i])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['好惨']\n",
      "['棉花', '肉松']\n",
      "['全过程']\n",
      "['杨澜', '國籍']\n",
      "['假蛋']\n",
      "['哈尔滨', '机场']\n",
      "['河北', '水灾']\n",
      "['罗一笑', '你']\n",
      "['假', '鸡蛋']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "source": [
    "from fastNLP import Vocabulary\n",
    "vocab=Vocabulary()\n",
    "vocab.from_dataset(data_train,field_name='raw_chars')\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Vocabulary(['王嘉尔', '热血', '街', '舞团', '或']...)"
      ]
     },
     "metadata": {},
     "execution_count": 158
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "source": [
    "vocab.index_dataset(data_train,field_name='raw_chars')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Vocabulary([3721, 3675, 1813, 7477, 353]...)"
      ]
     },
     "metadata": {},
     "execution_count": 149
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "source": [
    "print(data_train)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------------------------------------+--------+\n",
      "| raw_chars                                | target |\n",
      "+------------------------------------------+--------+\n",
      "| [3721, 3675, 1813, 7477, 3721, 353, 3... | 0      |\n",
      "| [97, 2, 974, 149, 32, 98, 167, 59, 18... | -1     |\n",
      "| [15642, 5691, 6, 9218, 7075, 2, 154, ... | 0      |\n",
      "| [536, 114, 167, 2, 127, 182, 35, 709,... | -1     |\n",
      "| [10, 21211, 2, 7642, 22, 2, 8062, 226... | 0      |\n",
      "| [18670, 83, 19401, 492, 378, 141, 506... | 1      |\n",
      "| [3652, 3620, 2607, 554, 225, 12, 5347... | 1      |\n",
      "| [145, 220, 3, 5, 47, 134, 2, 15174, 3... | 1      |\n",
      "| [270, 1200, 77, 39, 1692, 24846, 8876... | 1      |\n",
      "| [1441, 4318, 129, 4, 7, 175, 5, 274, ... | 0      |\n",
      "| [7643, 2, 268, 10396, 2, 7171, 15175,... | 0      |\n",
      "| [88771, 1641, 16692, 2, 121, 45732, 1... | 0      |\n",
      "| ...                                      | ...    |\n",
      "+------------------------------------------+--------+\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "source": [
    "import torch\n",
    "from fastNLP.embeddings import BertEmbedding\n",
    "from fastNLP import Vocabulary\n",
    "embed=BertEmbedding(vocab)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 1.02M/408M [00:00<01:16, 5.30MB/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "http://download.fastnlp.top/embedding/bert-base-uncased.zip not found in cache, downloading to /var/folders/34/5ls3qb4j0rl0kbsw8j22d1_w0000gn/T/tmpsb0cxjkp\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 408M/408M [01:02<00:00, 6.55MB/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Finish download from http://download.fastnlp.top/embedding/bert-base-uncased.zip\n",
      "Copy file to /Users/wumozhou/.fastNLP/embedding/bert-base-uncased\n",
      "loading vocabulary file /Users/wumozhou/.fastNLP/embedding/bert-base-uncased/vocab.txt\n",
      "Load pre-trained BERT parameters from file /Users/wumozhou/.fastNLP/embedding/bert-base-uncased/pytorch_model.bin.\n",
      "Bert Model will return 1 layers (layer-0 is embedding result): [-1]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "source": [
    "word1=torch.LongTensor(vocab.to_index(\"男\"))\n",
    "word2=torch.LongTensor(vocab.to_index(\"女\"))\n",
    "word3=torch.LongTensor(vocab.to_index(\"热血\"))\n",
    "#word1=embed(word1)\n",
    "#word2=embed(word2)\n",
    "#word3=embed(word3)\n",
    "print(word1.size())\n",
    "print(word2.size())\n",
    "print(word3.size())\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([475])\n",
      "torch.Size([333])\n",
      "torch.Size([3675])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 使用情绪分析流水线\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "ans=classifier('We are very happy to introduce pipeline to the transformers repository.')\n",
    "print(ans)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "7dd44356432448c335aab49bb4a35eec45732cd176bb87a60d07cce9b4b0faba"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}