{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pickle\r\n",
    "dataset=open('split_dataset','rb')\r\n",
    "dataset=pickle.load(dataset)\r\n",
    "testset=open('split_testset','rb')\r\n",
    "testset=pickle.load(testset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def label_get(dataset):\r\n",
    "    labelset=[]\r\n",
    "    for index,row in dataset.iterrows():\r\n",
    "        labelset.append(dataset.loc[index,'label'])\r\n",
    "    return labelset\r\n",
    "\r\n",
    "labelset=label_get(dataset)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def pd_li_transform(dataset):\r\n",
    "    dataset_list=[]\r\n",
    "    for index,row in dataset.iterrows():\r\n",
    "        str2=\" \"\r\n",
    "        if dataset.loc[index,'comment_all']is None:\r\n",
    "            dataset_list.append(dataset.loc[index,'content'])\r\n",
    "        else:\r\n",
    "            dataset_list.append(dataset.loc[index,'content']+dataset.loc[index,'comment_all'])\r\n",
    "    return dataset_list\r\n",
    "testset_list=pd_li_transform(testset)\r\n",
    "dataset_list=pd_li_transform(dataset)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def list_deal(dataset,labelset):\r\n",
    "    labelset2=[]\r\n",
    "    dataset2=[]\r\n",
    "    for i in range(len(dataset)):\r\n",
    "        if (dataset[i]!=[]):\r\n",
    "            dataset2.append(dataset[i])\r\n",
    "            labelset2.append(labelset[i])\r\n",
    "    return dataset2,labelset2\r\n",
    "dataset_list,labelset=list_deal(dataset_list,labelset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def list_onehot(labelset):\r\n",
    "    newli=[]\r\n",
    "    for index in labelset:\r\n",
    "        if (index==-1):newli.append([-1.0,0.0,0.0])\r\n",
    "        if (index==0):newli.append([0.0,-1.0,0.0])\r\n",
    "        if (index==1):newli.append([0.0,0.0,-1.0])\r\n",
    "    return newli\r\n",
    "def list_float(labelset):\r\n",
    "    newli=[]\r\n",
    "    for index in labelset:\r\n",
    "        if (index==-1):newli.append(0.0)\r\n",
    "        if (index==0):newli.append(1.0)\r\n",
    "        if (index==1):newli.append(2.0)\r\n",
    "    return newli\r\n",
    "labelset3=list_float(labelset)\r\n",
    "labelset2=list_onehot(labelset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def li_str_transform(dataset_list):\r\n",
    "    dataset2=[]\r\n",
    "    for seq in dataset_list:\r\n",
    "        str1=' '.join(seq)\r\n",
    "        dataset2.append(str1)\r\n",
    "    return dataset2\r\n",
    "dataset_str=li_str_transform(dataset_list)\r\n",
    "testset_str=li_str_transform(testset_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "from fastNLP import DataSet\r\n",
    "data_init=DataSet({'raw_chars':dataset_list,'target':labelset})\r\n",
    "data_train=data_init[:35000]\r\n",
    "data_validation=data_init[35001:]\r\n",
    "data_test=DataSet({'raw_chars':testset_list})\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "from fastNLP import Vocabulary\r\n",
    "vocab=Vocabulary()\r\n",
    "vocab.from_dataset(data_train,field_name='raw_chars',no_create_entry_dataset=[data_validation])\r\n",
    "vocab.index_dataset(data_train,field_name='raw_chars')\r\n",
    "vocab.index_dataset(data_validation,field_name='raw_chars')\r\n",
    "vocab.index_dataset(data_test,field_name='raw_chars')\r\n",
    "target_vocab=Vocabulary(padding=None,unknown=None)\r\n",
    "target_vocab.from_dataset(data_train,field_name='target')\r\n",
    "target_vocab.index_dataset(data_train,field_name='target')\r\n",
    "target_vocab.index_dataset(data_validation,field_name='target')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Vocabulary([0, -1, 1]...)"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "data_train\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "+------------------------------------------+--------+\n",
       "| raw_chars                                | target |\n",
       "+------------------------------------------+--------+\n",
       "| [3721, 3675, 1813, 7477, 3721, 352, 3... | 2      |\n",
       "| [97, 2, 974, 149, 32, 98, 167, 59, 18... | 1      |\n",
       "| [15642, 5691, 6, 9217, 7075, 2, 154, ... | 2      |\n",
       "| [536, 114, 167, 2, 127, 182, 35, 709,... | 1      |\n",
       "| [10, 21211, 2, 7642, 22, 2, 8062, 226... | 2      |\n",
       "| [18670, 83, 19401, 492, 378, 141, 506... | 0      |\n",
       "| [3652, 3620, 2607, 554, 225, 12, 5346... | 0      |\n",
       "| [145, 220, 3, 5, 47, 134, 2, 15173, 3... | 0      |\n",
       "| [270, 1200, 77, 39, 1692, 24843, 8876... | 0      |\n",
       "| [1441, 4317, 129, 4, 7, 175, 5, 274, ... | 2      |\n",
       "| [7643, 2, 268, 10394, 2, 7171, 15174,... | 2      |\n",
       "| [88769, 1641, 16692, 2, 121, 45732, 1... | 2      |\n",
       "| ...                                      | ...    |\n",
       "+------------------------------------------+--------+"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "data_train.rename_field('raw_chars','words')\r\n",
    "data_validation.rename_field('raw_chars','words')\r\n",
    "data_train.set_input('words')\r\n",
    "data_train.set_target('target')\r\n",
    "data_validation.set_input('words')\r\n",
    "data_validation.set_target('target')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "+--------+------------------------------------------+\n",
       "| target | words                                    |\n",
       "+--------+------------------------------------------+\n",
       "| 1      | [143, 6, 1071, 73, 240, 700, 328, 441... |\n",
       "| 0      | [11692, 251, 39496, 2749, 59995, 34, ... |\n",
       "| 0      | [158503, 1188, 16448, 25969, 420, 2, ... |\n",
       "| 2      | [4159, 236, 518, 2, 153, 81620, 2, 69... |\n",
       "| 0      | [1093, 1586, 230, 199, 2424, 3228, 27... |\n",
       "| 0      | [683, 2707, 46321, 9399, 3, 51, 3, 54... |\n",
       "| 1      | [616, 64, 40, 229, 3, 794, 67, 1265, ... |\n",
       "| 0      | [328, 104, 1339, 3783, 1148, 2496, 31... |\n",
       "| 2      | [9841, 10021, 15083, 14882, 19515, 98... |\n",
       "| 0      | [5048, 368, 126, 19264, 626, 109, 209... |\n",
       "| 0      | [574, 3553, 120, 280, 128, 960, 1225,... |\n",
       "| 2      | [7836, 32, 5018, 921, 167, 1516, 135,... |\n",
       "| ...    | ...                                      |\n",
       "+--------+------------------------------------------+"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "print(data_validation)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+------------------------------------------+\n",
      "| target | words                                    |\n",
      "+--------+------------------------------------------+\n",
      "| 1      | [143, 6, 1071, 73, 240, 700, 328, 441... |\n",
      "| 0      | [11692, 251, 39496, 2749, 59995, 34, ... |\n",
      "| 0      | [158503, 1188, 16448, 25969, 420, 2, ... |\n",
      "| 2      | [4159, 236, 518, 2, 153, 81620, 2, 69... |\n",
      "| 0      | [1093, 1586, 230, 199, 2424, 3228, 27... |\n",
      "| 0      | [683, 2707, 46321, 9399, 3, 51, 3, 54... |\n",
      "| 1      | [616, 64, 40, 229, 3, 794, 67, 1265, ... |\n",
      "| 0      | [328, 104, 1339, 3783, 1148, 2496, 31... |\n",
      "| 2      | [9841, 10021, 15083, 14882, 19515, 98... |\n",
      "| 0      | [5048, 368, 126, 19264, 626, 109, 209... |\n",
      "| 0      | [574, 3553, 120, 280, 128, 960, 1225,... |\n",
      "| 2      | [7836, 32, 5018, 921, 167, 1516, 135,... |\n",
      "| ...    | ...                                      |\n",
      "+--------+------------------------------------------+\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "import torch\r\n",
    "from fastNLP.embeddings import BertEmbedding\r\n",
    "from fastNLP import Vocabulary\r\n",
    "from fastNLP.core.losses import MSELoss\r\n",
    "embed=BertEmbedding(vocab,model_dir_or_name='cn-wwm', include_cls_sep=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading vocabulary file C:\\Users\\LENOVO\\.fastNLP\\embedding\\bert-chinese-wwm\\vocab.txt\n",
      "Load pre-trained BERT parameters from file C:\\Users\\LENOVO\\.fastNLP\\embedding\\bert-chinese-wwm\\chinese_wwm_pytorch.bin.\n",
      "Bert Model will return 1 layers (layer-0 is embedding result): [-1]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "from fastNLP.models import BertForSequenceClassification\r\n",
    "model = BertForSequenceClassification(embed,num_labels=3)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "from fastNLP import Trainer, CrossEntropyLoss, AccuracyMetric, Adam\r\n",
    "\r\n",
    "\r\n",
    "device = 0 if torch.cuda.is_available() else 'cpu'\r\n",
    "trainer = Trainer(data_train, model,\r\n",
    "                  optimizer=Adam(model_params=model.parameters(), lr=2e-5),\r\n",
    "                  loss=CrossEntropyLoss(), device=device,\r\n",
    "                  batch_size=8, dev_data=data_validation,\r\n",
    "                  metrics=AccuracyMetric(), n_epochs=2, print_every=1)\r\n",
    "trainer.train()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input fields after batch(if batch size is 2):\n",
      "\twords: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2, 247]) \n",
      "target fields after batch(if batch size is 2):\n",
      "\ttarget: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) \n",
      "\n",
      "training epochs started 2021-11-26-01-36-39-787608\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "After split words into word pieces, the lengths of word pieces are longer than the maximum allowed sequence length:512 of bert. You can set `auto_truncate=True` for BertEmbedding to automatically truncate overlong input.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9856/4073957524.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                   \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_validation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                   metrics=AccuracyMetric(), n_epochs=2, print_every=1)\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\fastNLP\\core\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, load_best_model, on_exception, **kwargs)\u001b[0m\n\u001b[0;32m    665\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mon_exception\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'auto'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mCallbackException\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 667\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    668\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mon_exception\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raise'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\fastNLP\\core\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, load_best_model, on_exception, **kwargs)\u001b[0m\n\u001b[0;32m    656\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    659\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\fastNLP\\core\\trainer.py\u001b[0m in \u001b[0;36m_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    716\u001b[0m                     \u001b[1;31m# negative sampling; replace unknown; re-weight batch_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 718\u001b[1;33m                     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    719\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m                     \u001b[1;31m# edit prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\fastNLP\\core\\trainer.py\u001b[0m in \u001b[0;36m_data_forward\u001b[1;34m(self, network, x)\u001b[0m\n\u001b[0;32m    819\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_build_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_cast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 821\u001b[1;33m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    822\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    823\u001b[0m             raise TypeError(\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\fastNLP\\models\\bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mattr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfastNLP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOUTPUT\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \"\"\"\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         \u001b[0mcls_hidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls_hidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\fastNLP\\embeddings\\bert_embedding.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\fastNLP\\embeddings\\bert_embedding.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m    465\u001b[0m                         self._max_position_embeddings - 2)\n\u001b[0;32m    466\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 467\u001b[1;33m                     raise RuntimeError(\n\u001b[0m\u001b[0;32m    468\u001b[0m                         \u001b[1;34m\"After split words into word pieces, the lengths of word pieces are longer than the \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m                         \u001b[1;34mf\"maximum allowed sequence length:{self._max_position_embeddings} of bert. You can set \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: After split words into word pieces, the lengths of word pieces are longer than the maximum allowed sequence length:512 of bert. You can set `auto_truncate=True` for BertEmbedding to automatically truncate overlong input."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "DataSetIter(dataset, batch_size=batch_size, sampler=None)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "word1=torch.LongTensor(vocab.to_index(\"男\"))\r\n",
    "word2=torch.LongTensor(vocab.to_index(\"女\"))\r\n",
    "word3=torch.LongTensor(vocab.to_index(\"热血\"))\r\n",
    "#word1=embed(word1)\r\n",
    "#word2=embed(word2)\r\n",
    "#word3=embed(word3)\r\n",
    "print(word1.size())\r\n",
    "print(word2.size())\r\n",
    "print(word3.size())\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([475])\n",
      "torch.Size([333])\n",
      "torch.Size([3675])\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "interpreter": {
   "hash": "cc5f70855ac006f3de45a3cc3b9e7d8d53845e50458809cb162b0174266dec97"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}