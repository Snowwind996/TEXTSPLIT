{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "import pickle\n",
    "dataset=open('split_dataset','rb')\n",
    "dataset=pickle.load(dataset)\n",
    "testset=open('split_testset','rb')\n",
    "testset=pickle.load(testset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "def label_get(dataset):\n",
    "    labelset=[]\n",
    "    for index,row in dataset.iterrows():\n",
    "        labelset.append(dataset.loc[index,'label'])\n",
    "    return labelset\n",
    "labelset=label_get(dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "def pd_li_transform(dataset):\n",
    "    dataset_list=[]\n",
    "    for index,row in dataset.iterrows():\n",
    "        str2=\" \"\n",
    "        if dataset.loc[index,'comment_all']is None:\n",
    "            dataset_list.append(dataset.loc[index,'content'])\n",
    "        else:\n",
    "            dataset_list.append(dataset.loc[index,'content']+dataset.loc[index,'comment_all'])\n",
    "    return dataset_list\n",
    "testset_list=pd_li_transform(testset)\n",
    "dataset_list=pd_li_transform(dataset)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "def list_deal(dataset,labelset):\n",
    "    labelset2=[]\n",
    "    dataset2=[]\n",
    "    for i in range(len(dataset)):\n",
    "        if (dataset[i]!=[]):\n",
    "            dataset2.append(dataset[i])\n",
    "            labelset2.append(labelset[i])\n",
    "    return dataset2,labelset2\n",
    "dataset_list,labelset=list_deal(dataset_list,labelset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "def li_str_transform(dataset_list):\n",
    "    dataset2=[]\n",
    "    for seq in dataset_list:\n",
    "        str1=' '.join(seq)\n",
    "        dataset2.append(str1)\n",
    "    return dataset2\n",
    "dataset_str=li_str_transform(dataset_list)\n",
    "testset_str=li_str_transform(testset_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "from fastNLP import DataSet\n",
    "data_train=DataSet({'raw_chars':dataset_list,'target':labelset})\n",
    "data_validation=DataSet({'raw_chars':testset_list})\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "for i in range(0,len(dataset_list)):\n",
    "    if len(dataset_list[i])<3:print(dataset_list[i])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['好惨']\n",
      "['棉花', '肉松']\n",
      "['全过程']\n",
      "['杨澜', '國籍']\n",
      "['假蛋']\n",
      "['哈尔滨', '机场']\n",
      "['河北', '水灾']\n",
      "['罗一笑', '你']\n",
      "['假', '鸡蛋']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "from fastNLP import Vocabulary\n",
    "vocab=Vocabulary()\n",
    "vocab.from_dataset(data_train,field_name='raw_chars')\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Vocabulary(['王嘉尔', '热血', '街', '舞团', '或']...)"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "vocab.index_dataset(data_train,field_name='raw_chars')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Vocabulary(['王嘉尔', '热血', '街', '舞团', '或']...)"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "vocab.index_dataset(data_validation,field_name='raw_chars')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Vocabulary(['王嘉尔', '热血', '街', '舞团', '或']...)"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "data_train.rename_field('raw_chars','words')\n",
    "data_validation.rename_field('raw_chars','words')\n",
    "data_train.set_input('words')\n",
    "data_train.set_target('target')\n",
    "data_validation.set_input('words')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "+------------------------------------------------------------------------------+\n",
       "| words                                                                        |\n",
       "+------------------------------------------------------------------------------+\n",
       "| [483, 2707, 1756, 21233, 1944, 3, 1147, 4198, 1964, 53, 1282, 1756, 122, ... |\n",
       "| [1825, 48, 1027, 1644, 459, 2980, 4859, 26829, 5026, 1181, 2060, 240, 111... |\n",
       "| [579, 9, 244, 1809, 143, 1537, 7024, 110, 9, 209, 767, 457, 8754, 2, 266,... |\n",
       "| [98, 286, 600, 2998, 68, 3116, 3994, 1559, 4567, 1769, 1852, 339, 8, 98, ... |\n",
       "| [8265, 24630, 81933, 4154, 2473, 1167, 65, 9, 8265, 13, 517]                 |\n",
       "| [127, 182, 35, 709, 697, 422, 252, 98, 142, 194, 231, 32, 91, 6, 287, 279... |\n",
       "| [10431, 913, 623, 3283, 6, 2017, 47067, 102, 19701, 5851, 3, 6171, 4161, ... |\n",
       "| [932, 55, 20045, 53, 752, 457, 2394, 942, 242, 3254, 336, 143, 2406, 2394... |\n",
       "| [252, 9, 335, 362, 203, 98, 142, 194, 231, 32, 91, 6, 287, 279, 188, 133,... |\n",
       "| [252, 98, 142, 194, 231, 32, 91, 6, 82257, 3292, 6344, 1954, 188, 133, 13... |\n",
       "| [636, 666, 782, 7, 214, 2064, 847, 9035, 20, 345, 1248, 2075, 2176, 4713,... |\n",
       "| [374, 6, 230, 1024, 7600, 28499, 1357, 20438, 376, 104, 534, 887, 1622, 8... |\n",
       "| ...                                                                          |\n",
       "+------------------------------------------------------------------------------+"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "data_train.print_field_meta()\n",
    "data_validation.print_field_meta()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------+--------+-------+\n",
      "| field_names | target | words |\n",
      "+-------------+--------+-------+\n",
      "|   is_input  | False  |  True |\n",
      "|  is_target  |  True  | False |\n",
      "| ignore_type | False  | False |\n",
      "|  pad_value  |   0    |   0   |\n",
      "+-------------+--------+-------+\n",
      "+-------------+-------+\n",
      "| field_names | words |\n",
      "+-------------+-------+\n",
      "|   is_input  |  True |\n",
      "|  is_target  | False |\n",
      "| ignore_type | False |\n",
      "|  pad_value  |   0   |\n",
      "+-------------+-------+\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>field_names</th>\n",
       "            <th>words</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>is_input</td>\n",
       "            <td>True</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>is_target</td>\n",
       "            <td>False</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>ignore_type</td>\n",
       "            <td>False</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>pad_value</td>\n",
       "            <td>0</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+-------------+-------+\n",
       "| field_names | words |\n",
       "+-------------+-------+\n",
       "|   is_input  |  True |\n",
       "|  is_target  | False |\n",
       "| ignore_type | False |\n",
       "|  pad_value  |   0   |\n",
       "+-------------+-------+"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "data_train"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "+--------+------------------------------------------+\n",
       "| target | words                                    |\n",
       "+--------+------------------------------------------+\n",
       "| 0      | [3721, 3675, 1813, 7477, 3721, 353, 3... |\n",
       "| -1     | [97, 2, 974, 149, 32, 98, 167, 59, 18... |\n",
       "| 0      | [15642, 5691, 6, 9218, 7075, 2, 154, ... |\n",
       "| -1     | [536, 114, 167, 2, 127, 182, 35, 709,... |\n",
       "| 0      | [10, 21211, 2, 7642, 22, 2, 8062, 226... |\n",
       "| 1      | [18670, 83, 19401, 492, 378, 141, 506... |\n",
       "| 1      | [3652, 3620, 2607, 554, 225, 12, 5347... |\n",
       "| 1      | [145, 220, 3, 5, 47, 134, 2, 15174, 3... |\n",
       "| 1      | [270, 1200, 77, 39, 1692, 24846, 8876... |\n",
       "| 0      | [1441, 4318, 129, 4, 7, 175, 5, 274, ... |\n",
       "| 0      | [7643, 2, 268, 10396, 2, 7171, 15175,... |\n",
       "| 0      | [88771, 1641, 16692, 2, 121, 45732, 1... |\n",
       "| ...    | ...                                      |\n",
       "+--------+------------------------------------------+"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "import torch\n",
    "from fastNLP.embeddings import BertEmbedding\n",
    "from fastNLP import Vocabulary\n",
    "from fastNLP.core.losses import MSELoss\n",
    "embed=BertEmbedding(vocab,model_dir_or_name='cn-wwm', include_cls_sep=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading vocabulary file /Users/wumozhou/.fastNLP/embedding/bert-chinese-wwm/vocab.txt\n",
      "Load pre-trained BERT parameters from file /Users/wumozhou/.fastNLP/embedding/bert-chinese-wwm/chinese_wwm_pytorch.bin.\n",
      "Bert Model will return 1 layers (layer-0 is embedding result): [-1]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "model = BertForSequenceClassification(embed,num_labels=3)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "device = 0 if torch.cuda.is_available() else 'cpu'\n",
    "trainer = Trainer(train_data=data_train, model=model,\n",
    "                  optimizer=Adam(model_params=model.parameters(), lr=2e-5),\n",
    "                  loss=MSELoss(), device=device,\n",
    "                  batch_size=8, dev_data=data_train,\n",
    "                  metrics=AccuracyMetric(), n_epochs=2, print_every=1)\n",
    "trainer.train()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input fields after batch(if batch size is 2):\n",
      "\twords: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2, 247]) \n",
      "target fields after batch(if batch size is 2):\n",
      "\ttarget: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) \n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/wumozhou/opt/anaconda3/lib/python3.8/site-packages/fastNLP/core/losses.py:294: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input=pred, target=target, reduction=self.reduction)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/34/5ls3qb4j0rl0kbsw8j22d1_w0000gn/T/ipykernel_26721/3112900999.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m trainer = Trainer(train_data=data_train, model=model,\n\u001b[0m\u001b[1;32m      3\u001b[0m                   \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                   \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                   \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/fastNLP/core/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train_data, model, optimizer, loss, batch_size, sampler, drop_last, update_every, num_workers, n_epochs, print_every, dev_data, metrics, metric_key, validate_every, save_path, use_tqdm, device, callbacks, check_code_level, fp16, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m                     \u001b[0mcheck_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m             _check_code(dataset=train_data, model=self.model, losser=losser, forward_func=self._forward_func, metrics=metrics,\n\u001b[0m\u001b[1;32m    560\u001b[0m                         \u001b[0mdev_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_code_level\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m                         batch_size=check_batch_size)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/fastNLP/core/trainer.py\u001b[0m in \u001b[0;36m_check_code\u001b[0;34m(dataset, model, losser, metrics, forward_func, batch_size, dev_data, metric_key, check_level)\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;31m# loss check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m             \u001b[0;31m# check loss output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/fastNLP/core/losses.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pred_dict, target_dict, check)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mrefined_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_build_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmapped_pred_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmapped_target_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrefined_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/fastNLP/core/losses.py\u001b[0m in \u001b[0;36mget_loss\u001b[0;34m(self, pred, target)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3109\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3111\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3112\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "DataSetIter(dataset, batch_size=batch_size, sampler=None)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "word1=torch.LongTensor(vocab.to_index(\"男\"))\n",
    "word2=torch.LongTensor(vocab.to_index(\"女\"))\n",
    "word3=torch.LongTensor(vocab.to_index(\"热血\"))\n",
    "#word1=embed(word1)\n",
    "#word2=embed(word2)\n",
    "#word3=embed(word3)\n",
    "print(word1.size())\n",
    "print(word2.size())\n",
    "print(word3.size())\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([475])\n",
      "torch.Size([333])\n",
      "torch.Size([3675])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from fastNLP.models import BertForSequenceClassification\n",
    "from fastNLP import Trainer, CrossEntropyLoss, AccuracyMetric, Adam\n",
    "model = BertForSequenceClassification(embed, len(data_train.get_vocab('target')))"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/34/5ls3qb4j0rl0kbsw8j22d1_w0000gn/T/ipykernel_26721/3150605597.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfastNLP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfastNLP\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAccuracyMetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "7dd44356432448c335aab49bb4a35eec45732cd176bb87a60d07cce9b4b0faba"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}